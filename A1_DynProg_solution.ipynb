{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMeh2Oo8M_Wd"
      },
      "source": [
        "By: Gediyon M. Girma (1010824360) - Intro to RL (Summer 2024)\n",
        "\n",
        "Assignment A1: Dynamic Programming\n",
        "\n",
        "**Question**: What is the purpose of OpenAI gyms and how is it going to help us in our RL education?\n",
        "\n",
        "**Answer**: the purpose of openAI gyms is to provide a platform to benchmark and compare newly developed AI algorithms to a well-known algorithms. In our RL educaation, we can use\n",
        "the gym module to practice and learn about RL by modifying the existing benchmark problems.:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HsGqubiDkQnd"
      },
      "outputs": [],
      "source": [
        "import gym\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7iS40R9okStg",
        "outputId": "204c7cdf-785d-4c37-e7bc-0c4a5b2d90df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:542: UserWarning: \u001b[33mWARN: Overriding environment FrozenLakeNotSlippery-v0\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {spec.id}\")\n"
          ]
        }
      ],
      "source": [
        "gym.envs.register(\n",
        "    id='FrozenLakeNotSlippery-v0',\n",
        "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
        "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
        "    max_episode_steps=100,\n",
        "    reward_threshold=0.74\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FVX1AjRWkueO",
        "outputId": "8dc9bb87-c65a-4e6b-be25-b7d05f5e9434"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{0: {0: [(1.0, 0, 0.0, False)],\n",
              "  1: [(1.0, 4, 0.0, False)],\n",
              "  2: [(1.0, 1, 0.0, False)],\n",
              "  3: [(1.0, 0, 0.0, False)]},\n",
              " 1: {0: [(1.0, 0, 0.0, False)],\n",
              "  1: [(1.0, 5, 0.0, True)],\n",
              "  2: [(1.0, 2, 0.0, False)],\n",
              "  3: [(1.0, 1, 0.0, False)]},\n",
              " 2: {0: [(1.0, 1, 0.0, False)],\n",
              "  1: [(1.0, 6, 0.0, False)],\n",
              "  2: [(1.0, 3, 0.0, False)],\n",
              "  3: [(1.0, 2, 0.0, False)]},\n",
              " 3: {0: [(1.0, 2, 0.0, False)],\n",
              "  1: [(1.0, 7, 0.0, True)],\n",
              "  2: [(1.0, 3, 0.0, False)],\n",
              "  3: [(1.0, 3, 0.0, False)]},\n",
              " 4: {0: [(1.0, 4, 0.0, False)],\n",
              "  1: [(1.0, 8, 0.0, False)],\n",
              "  2: [(1.0, 5, 0.0, True)],\n",
              "  3: [(1.0, 0, 0.0, False)]},\n",
              " 5: {0: [(1.0, 5, 0, True)],\n",
              "  1: [(1.0, 5, 0, True)],\n",
              "  2: [(1.0, 5, 0, True)],\n",
              "  3: [(1.0, 5, 0, True)]},\n",
              " 6: {0: [(1.0, 5, 0.0, True)],\n",
              "  1: [(1.0, 10, 0.0, False)],\n",
              "  2: [(1.0, 7, 0.0, True)],\n",
              "  3: [(1.0, 2, 0.0, False)]},\n",
              " 7: {0: [(1.0, 7, 0, True)],\n",
              "  1: [(1.0, 7, 0, True)],\n",
              "  2: [(1.0, 7, 0, True)],\n",
              "  3: [(1.0, 7, 0, True)]},\n",
              " 8: {0: [(1.0, 8, 0.0, False)],\n",
              "  1: [(1.0, 12, 0.0, True)],\n",
              "  2: [(1.0, 9, 0.0, False)],\n",
              "  3: [(1.0, 4, 0.0, False)]},\n",
              " 9: {0: [(1.0, 8, 0.0, False)],\n",
              "  1: [(1.0, 13, 0.0, False)],\n",
              "  2: [(1.0, 10, 0.0, False)],\n",
              "  3: [(1.0, 5, 0.0, True)]},\n",
              " 10: {0: [(1.0, 9, 0.0, False)],\n",
              "  1: [(1.0, 14, 0.0, False)],\n",
              "  2: [(1.0, 11, 0.0, True)],\n",
              "  3: [(1.0, 6, 0.0, False)]},\n",
              " 11: {0: [(1.0, 11, 0, True)],\n",
              "  1: [(1.0, 11, 0, True)],\n",
              "  2: [(1.0, 11, 0, True)],\n",
              "  3: [(1.0, 11, 0, True)]},\n",
              " 12: {0: [(1.0, 12, 0, True)],\n",
              "  1: [(1.0, 12, 0, True)],\n",
              "  2: [(1.0, 12, 0, True)],\n",
              "  3: [(1.0, 12, 0, True)]},\n",
              " 13: {0: [(1.0, 12, 0.0, True)],\n",
              "  1: [(1.0, 13, 0.0, False)],\n",
              "  2: [(1.0, 14, 0.0, False)],\n",
              "  3: [(1.0, 9, 0.0, False)]},\n",
              " 14: {0: [(1.0, 13, 0.0, False)],\n",
              "  1: [(1.0, 14, 0.0, False)],\n",
              "  2: [(1.0, 15, 1.0, True)],\n",
              "  3: [(1.0, 10, 0.0, False)]},\n",
              " 15: {0: [(1.0, 15, 0, True)],\n",
              "  1: [(1.0, 15, 0, True)],\n",
              "  2: [(1.0, 15, 0, True)],\n",
              "  3: [(1.0, 15, 0, True)]}}"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create the gridworld-like environment\n",
        "env=gym.make('FrozenLakeNotSlippery-v0', render_mode='human')\n",
        "# Let's look at the model of the environment (i.e., P):\n",
        "env.env.P\n",
        "# Question: what is the data in this structure saying? Relate this to the course\n",
        "# presentation of P\n",
        "\n",
        "# Answer: env.env.p represents the model of the environment that defines the dynamics of MDP\n",
        "#  we have represented env.env.p as p(s',r|s,a) in our lecure and in the text book\n",
        "\n",
        "# The result below show p represented as a dictioinary. Current state s is the key and\n",
        "# a second nested dictionary as a value. The nested dictionary contains actions that can\n",
        "# be taken at that state as a key with a value of [(probability, next_state, reward, done)]\n",
        "#  where probability: is the probability of getting the next_state and reward.\n",
        "#        next_state: is the next state after taking the action and\n",
        "#        reward is the reward recieved for taking the action\n",
        "#        done: is a boolean indicating whether the episode has ended after the transition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gyn_w3ulkyZI",
        "outputId": "f9d228c9-45ba-498b-b5cc-1325f3c390e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discrete(16)\n"
          ]
        }
      ],
      "source": [
        "# Now let's investigate the observation space (i.e., S using our nomenclature),\n",
        "# and confirm we see it is a discrete space with 16 locations\n",
        "print(env.observation_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zND5ArI8k_qQ",
        "outputId": "c39fb6ff-d257-4f1f-8643-ffcee0bf8714"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16\n"
          ]
        }
      ],
      "source": [
        "stateSpaceSize = env.observation_space.n\n",
        "print(stateSpaceSize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_tp9YzRljnj",
        "outputId": "b2a7cd7e-0c2e-44c2-fce5-a437a4a52f6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discrete(4)\n"
          ]
        }
      ],
      "source": [
        "# Now let's investigate the action space (i.e., A) for the agent->environment\n",
        "# channel\n",
        "print(env.action_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFGNZNowluz2",
        "outputId": "301364d2-d25f-4cc2-9430-7b0acc1a3cec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample from S: 13  ...  sample from A: 2\n",
            "sample from S: 9  ...  sample from A: 2\n",
            "sample from S: 2  ...  sample from A: 0\n",
            "sample from S: 15  ...  sample from A: 3\n",
            "sample from S: 0  ...  sample from A: 3\n",
            "sample from S: 11  ...  sample from A: 3\n",
            "sample from S: 12  ...  sample from A: 0\n",
            "sample from S: 11  ...  sample from A: 2\n",
            "sample from S: 14  ...  sample from A: 0\n"
          ]
        }
      ],
      "source": [
        "# The gym environment has ...sample() functions that allow us to sample\n",
        "# from the above spaces:\n",
        "for g in range(1,10,1):\n",
        "  print(\"sample from S:\",env.observation_space.sample(),\" ... \",\"sample from A:\",env.action_space.sample())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOQL5JxsmcEd"
      },
      "outputs": [],
      "source": [
        "# The enviroment also provides a helper to render (visualize) the environment\n",
        "env.reset()\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLV6e43mmwx1",
        "outputId": "e963722a-0624-4777-d48a-c6f989dc5397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the action as an integer from 0 to 4  (or exit): \n",
            "2\n",
            "--> The result of taking action 2 is:\n",
            "     S= 1\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "     compute= False\n",
            "Enter the action as an integer from 0 to 4  (or exit): \n",
            "2\n",
            "--> The result of taking action 2 is:\n",
            "     S= 2\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "     compute= False\n",
            "Enter the action as an integer from 0 to 4  (or exit): \n",
            "1\n",
            "--> The result of taking action 1 is:\n",
            "     S= 6\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "     compute= False\n",
            "Enter the action as an integer from 0 to 4  (or exit): \n",
            "1\n",
            "--> The result of taking action 1 is:\n",
            "     S= 10\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "     compute= False\n",
            "Enter the action as an integer from 0 to 4  (or exit): \n",
            "1\n",
            "--> The result of taking action 1 is:\n",
            "     S= 14\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "     compute= False\n",
            "Enter the action as an integer from 0 to 4  (or exit): \n",
            "2\n",
            "--> The result of taking action 2 is:\n",
            "     S= 15\n",
            "     R= 1.0\n",
            "     p= {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "     compute= True\n",
            "Enter the action as an integer from 0 to 4  (or exit): \n",
            "exit\n"
          ]
        }
      ],
      "source": [
        "# We can act as the agent, by selecting actions and stepping the environment\n",
        "# through time to see its responses to our actions\n",
        "env.reset()\n",
        "exitCommand=False\n",
        "while not(exitCommand):\n",
        "  env.render()\n",
        "  print(\"Enter the action as an integer from 0 to\",env.action_space.n,\" (or exit): \")\n",
        "  userInput=input()\n",
        "  if userInput==\"exit\":\n",
        "    break\n",
        "  action=int(userInput)\n",
        "  (observation, reward, compute, probability) = env.step(action)\n",
        "  print(\"--> The result of taking action\",action,\"is:\")\n",
        "  print(\"     S=\",observation)\n",
        "  print(\"     R=\",reward)\n",
        "  print(\"     p=\",probability)\n",
        "  print(\"     compute=\",compute)\n",
        "  # compute is a boolean lag indicating whether the episode has ended.\n",
        "  # True means the episode is over, either because a terminal state is reached or because a time limit has been exceeded.\n",
        "  env.render()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tBpeiuRnyih"
      },
      "outputs": [],
      "source": [
        "# Question: draw a table indicating the correspondence between the action\n",
        "# you input (a number) and the logic action performed.\n",
        "\n",
        "# Answer:\n",
        "# Action\tAction logic\n",
        "# 0\tMove left\n",
        "# 1\tMove down\n",
        "# 2\tMove right\n",
        "# 3\tMove up\n",
        "\n",
        "\n",
        "\n",
        "# Question: draw a table that illustrates what the symbols on the render image\n",
        "# mean?\n",
        "\n",
        "# Answer:\n",
        "# let's denote the symbols on the render image as:\n",
        "# S: starting point, safe\n",
        "# F: frozen surface, safe\n",
        "# H: hole, fall to your doom\n",
        "# G: goal, the denstination\n",
        "\n",
        "# The following is how it looks when the environment is visulalized in a table form:\n",
        "# S\tF\tF\tF\n",
        "# F\tH\tF\tH\n",
        "# F\tF\tF\tH\n",
        "# H\tF\tF\tG\n",
        "\n",
        "\n",
        "# Question: Explain what the objective of the agent is in this environment?\n",
        "\n",
        "# Answer:\n",
        "\n",
        "# The objective of the agent is to find a path from the starting point to the goal\n",
        "#  while avoiding falling into the holes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmRwGwPoqw0F",
        "outputId": "bae1d4f9-125b-4902-a2dd-af6c1fa9dbb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 0\n",
            "State: 0 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 4 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 4 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 1 Reward: 0.0 Done: True p(s',r|s,a):  {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "Episode: 1\n",
            "State: 0 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 4 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 4 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 8 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 9 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 10 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 9 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 10 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 14 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 13 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 13 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 9 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 13 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 9 Action: 3 Reward: 0.0 Done: True p(s',r|s,a):  {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "Episode: 2\n",
            "State: 0 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 2 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 6 Action: 2 Reward: 0.0 Done: True p(s',r|s,a):  {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "Episode: 3\n",
            "State: 0 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 1 Reward: 0.0 Done: True p(s',r|s,a):  {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "Episode: 4\n",
            "State: 0 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 4 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 4 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 1 Reward: 0.0 Done: True p(s',r|s,a):  {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "Episode: 5\n",
            "State: 0 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 2 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 2 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 1 Reward: 0.0 Done: True p(s',r|s,a):  {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "Episode: 6\n",
            "State: 0 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 4 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 4 Action: 2 Reward: 0.0 Done: True p(s',r|s,a):  {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "Episode: 7\n",
            "State: 0 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 4 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 4 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 8 Action: 1 Reward: 0.0 Done: True p(s',r|s,a):  {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "Episode: 8\n",
            "State: 0 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 2 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 6 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 2 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 2 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 2 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 2 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 1 Reward: 0.0 Done: True p(s',r|s,a):  {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "Episode: 9\n",
            "State: 0 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 4 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 4 Action: 2 Reward: 0.0 Done: True p(s',r|s,a):  {'prob': 1.0, 'TimeLimit.truncated': False}\n"
          ]
        }
      ],
      "source": [
        "# Practical: Code up an AI that will employ random action selection in order\n",
        "# to drive the agent. Test this random action selection agent with the\n",
        "# above environment (i.e., code up a loop as I did above, but instead\n",
        "# of taking input from a human user, take it from the AI you coded).\n",
        "import random\n",
        "\n",
        "# Initialize the random action selection AI\n",
        "random_ai = lambda: random.randint(0, env.action_space.n - 1)\n",
        "\n",
        "# Run the AI for a number of episodes\n",
        "episodes = 10\n",
        "for episode in range(episodes):\n",
        "  # Reset the environment\n",
        "  state = env.reset()\n",
        "  print(\"Episode:\", episode)\n",
        "\n",
        "  # Run the episode until done\n",
        "  done = False\n",
        "  while not done:\n",
        "    # Render the environment\n",
        "    env.render()\n",
        "\n",
        "    # Select a random action\n",
        "    action = random_ai()\n",
        "\n",
        "    # Take the action and observe the next state, reward, and done flag\n",
        "    next_state, reward, done, probability = env.step(action)\n",
        "\n",
        "    # Print the state, action, reward, and done flag\n",
        "    print(\"State:\", state, \"Action:\", action, \"Reward:\", reward, \"Done:\", done, \"p(s',r|s,a): \", probability)\n",
        "\n",
        "    # Update the state\n",
        "    state = next_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgmGdR97dasB",
        "outputId": "74039f83-1088-40cb-9181-87572aa7ac8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 0\n",
            "State: 0 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 4 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 4 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 4 Action: 2 Reward: 0.0 Done: True p(s',r|s,a):  {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "Episode: 1\n",
            "State: 0 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 2 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 2 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 2 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 4 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 4 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 4 Action: 2 Reward: 0.0 Done: True p(s',r|s,a):  {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "Episode: 2\n",
            "State: 0 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 4 Action: 2 Reward: 0.0 Done: True p(s',r|s,a):  {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "Episode: 3\n",
            "State: 0 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 4 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 8 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 9 Action: 3 Reward: 0.0 Done: True p(s',r|s,a):  {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "Episode: 4\n",
            "State: 0 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 2 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 3 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 2 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 2 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 3 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 3 Action: 1 Reward: 0.0 Done: True p(s',r|s,a):  {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "Episode: 5\n",
            "State: 0 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 2 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 1 Reward: 0.0 Done: True p(s',r|s,a):  {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "Episode: 6\n",
            "State: 0 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 4 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 2 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 3 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 3 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 2 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 6 Action: 2 Reward: 0.0 Done: True p(s',r|s,a):  {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "Episode: 7\n",
            "State: 0 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 4 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 4 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 8 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 8 Action: 1 Reward: 0.0 Done: True p(s',r|s,a):  {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "Episode: 8\n",
            "State: 0 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 4 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 4 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 1 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 4 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 3 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 1 Reward: 0.0 Done: True p(s',r|s,a):  {'prob': 1.0, 'TimeLimit.truncated': False}\n",
            "Episode: 9\n",
            "State: 0 Action: 0 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 0 Action: 2 Reward: 0.0 Done: False p(s',r|s,a):  {'prob': 1.0}\n",
            "State: 1 Action: 1 Reward: 0.0 Done: True p(s',r|s,a):  {'prob': 1.0, 'TimeLimit.truncated': False}\n"
          ]
        }
      ],
      "source": [
        "# Now towards dynamic programming. Note that env.env.P has the model\n",
        "# of the environment.\n",
        "#\n",
        "# Question: How would you represent the agent's policy function and value function?\n",
        "\n",
        "# Answer:\n",
        "\n",
        "# the policy function can be represented as a table, with a dictionary data structure where the keys are\n",
        "# the state and values are the probabilities of taking the actions [0, 1, 2, 3].\n",
        "# the policy can be initialized by giving equal probabillities to each action as follows:\n",
        "\n",
        "policy = {}\n",
        "\n",
        "\n",
        "for s in range(env.observation_space.n):\n",
        "  policy[s] = [0.25, 0.25, 0.25, 0.25] # initialize policy to equal probabilities for each action\n",
        "\n",
        "\n",
        "\n",
        "# Practical: revise the above AI solver to use a policy function in which you\n",
        "# code the random action selections in the policy function. Test this.\n",
        "\n",
        "\n",
        "# Answer:\n",
        "\n",
        "# Run the AI for a number of episodes\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "episodes = 10\n",
        "for episode in range(episodes):\n",
        "  # Reset the environment\n",
        "  state = env.reset()\n",
        "  print(\"Episode:\", episode)\n",
        "\n",
        "  # Run the episode until done\n",
        "  done = False\n",
        "  while not done:\n",
        "    # Render the environment\n",
        "    env.render()\n",
        "\n",
        "    # Select greedy action from a policy\n",
        "    max_value = max(policy[int(state)])\n",
        "    max_indices = [i for i, x in enumerate(policy[int(state)]) if x == max_value]\n",
        "    action = random.choice(max_indices)\n",
        "\n",
        "\n",
        "    # Take the action and observe the next state, reward, and done flag\n",
        "    next_state, reward, done, probability = env.step(action)\n",
        "\n",
        "    # Print the state, action, reward, and done flag\n",
        "    print(\"State:\", state, \"Action:\", action, \"Reward:\", reward, \"Done:\", done, \"p(s',r|s,a): \", probability)\n",
        "\n",
        "    # Update the state\n",
        "    state = next_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LksCBPcGcZo7",
        "outputId": "4ba6aed7-fbc7-43fd-c382-255d5585fe1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "policy evaluation finished! Here is the value function:  {0: 0.008992601045053505, 1: 0.008907737917843724, 2: 0.011323787429501159, 3: 0.009495944460556967, 4: 0.010272122157172173, 5: 0.006762384116579314, 6: 0.014259413669576337, 7: 0.008166672689292876, 8: 0.018719313476692694, 9: 0.06469299491755101, 10: 0.08799491103525386, 11: 0.007516063543337573, 12: 0.005937591967474624, 13: 0.14344833175866661, 14: 0.30016056009704484, 15: 0.0}\n"
          ]
        }
      ],
      "source": [
        "# Practical: Code the C-4 Policy Evaluation (Prediction) algorithm. You may use\n",
        "# either the inplace or ping-pong buffer (as described in the lecture). Now\n",
        "# randomly initialize your policy function, and compute its value function.\n",
        "# Report your results: policy and value function. Ensure your prediction\n",
        "# algo reports how many iterations it took.\n",
        "#\n",
        "\n",
        "# Answer:\n",
        "import random\n",
        "\n",
        "# randomly initializing the policy function\n",
        "\n",
        "# defining a function to randomly initialize the polilcy for each state\n",
        "def initialize_probabilities():\n",
        "    # Step 1: Generate four random numbers\n",
        "    rand_nums = [random.random() for _ in range(4)]\n",
        "\n",
        "    # Step 2: Calculate the sum of these numbers\n",
        "    total = sum(rand_nums)\n",
        "\n",
        "    # Step 3: Normalize the numbers to get probabilities\n",
        "    probabilities = [num / total for num in rand_nums]\n",
        "\n",
        "    return probabilities\n",
        "\n",
        "policy = {}\n",
        "\n",
        "for i in range(env.observation_space.n):\n",
        "  policy[i] = initialize_probabilities() # initialize policy to equal probabilities for each action\n",
        "\n",
        "\n",
        "# policy evaluation\n",
        "threshold = 0.001\n",
        "\n",
        "# initialize the vlaue function arbitrarily\n",
        "\n",
        "state_value = {}\n",
        "\n",
        "\n",
        "for i in range(env.observation_space.n - 1):\n",
        "  state_value[i] = random.random() # assign random state value for each state\n",
        "state_value[env.observation_space.n - 1] = 0  # assign the goal state as 0 since it's a terminal state\n",
        "\n",
        "gamma = 0.9  # discount factor\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "  max_delta = 0\n",
        "  delta_s = []\n",
        "  # evaluating the state value function\n",
        "  for state in state_value:  # loop through each state\n",
        "    new_s_value = 0\n",
        "    for action, action_prob in enumerate(policy[state]):  #Looping throught the actions in the policy\n",
        "      action_value = 0\n",
        "      for prob, next_state, reward, _  in env.env.P[state][action]: #looping through the next state probabilities and rewards\n",
        "        action_value += prob * (reward + gamma * state_value[next_state]) # calcluating the state action value q(s,a)\n",
        "      new_s_value += action_prob * action_value # calculating the state value from the action values\n",
        "\n",
        "    delta_s.append(abs(state_value[state] - new_s_value)) # recording the difference between the old and new state value\n",
        "    state_value[state] = new_s_value # updating the state value\n",
        "\n",
        "  max_delta = max(delta_s)\n",
        "  if max_delta < threshold: # checking if the maximum difference is less than the threshold\n",
        "    done = True # if, exit the loop\n",
        "\n",
        "print(\"policy evaluation finished! Here is the value function: \", state_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "smXQ4rZNCsqn",
        "outputId": "7d30f5fb-b346-4661-d375-f9d754fda9ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "policy evaluation finished! Here is the action value function q:  {0: {0: 0.18596531931048765, 1: 0.4147354669067011, 2: 0.4756025611565413, 3: 0.18596531931048765}, 1: {0: 0.25753858452772266, 1: 0.6619378962151853, 2: 0.46191993504343754, 3: 0.4756025611565413}, 2: {0: 0.4383086729088269, 1: 0.5436836696752273, 2: 0.4295606416324416, 3: 0.46191993504343754}, 3: {0: 0.43455504877766155, 1: 0.6562303530558601, 2: 0.4295606416324416, 3: 0.4295606416324416}, 4: {0: 0.4147354669067011, 1: 0.4771937661186661, 2: 0.6619378962151853, 3: 0.25753858452772266}, 5: {0: 0.6619378962151853, 1: 0.6619378962151853, 2: 0.6619378962151853, 3: 0.6619378962151853}, 6: {0: 0.5957441065936667, 1: 0.40913838627644344, 2: 0.6562303530558601, 3: 0.43455504877766155}, 7: {0: 0.6562303530558601, 1: 0.6562303530558601, 2: 0.6562303530558601, 3: 0.6562303530558601}, 8: {0: 0.4771937661186661, 1: 0.6102686791163724, 2: 0.4636638986445238, 3: 0.4172575437413748}, 9: {0: 0.4226447981904122, 1: 0.48061224930301766, 2: 0.40913838627644344, 3: 0.5957441065936667}, 10: {0: 0.41775097017170254, 1: 0.5484631446221471, 2: 0.30537070810450834, 3: 0.49486265480188235}, 11: {0: 0.30537070810450834, 1: 0.30537070810450834, 2: 0.30537070810450834, 3: 0.30537070810450834}, 12: {0: 0.6102686791163724, 1: 0.6102686791163724, 2: 0.6102686791163724, 3: 0.6102686791163724}, 13: {0: 0.5492418112047351, 1: 0.48061224930301766, 2: 0.5484631446221471, 3: 0.41775097017170254}, 14: {0: 0.4446344072779908, 1: 0.5484631446221471, 2: 1.0, 3: 0.3753388441743732}, 15: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}}\n"
          ]
        }
      ],
      "source": [
        "# (Optional): Repeat the above for q.\n",
        "\n",
        "# initializing our policy, action-value and state-value functions\n",
        "q = {}\n",
        "\n",
        "for state in range(env.observation_space.n):\n",
        "  av ={}\n",
        "  for action in range(env.action_space.n):\n",
        "    av[action] = random.random() # assign random state value for each action value\n",
        "  q[state] = av # initialize the action-value function\n",
        "  policy[state] = initialize_probabilities() # initialize policy to equal probabilities for each action\n",
        "  state_value[state] = random.random() # assign random state value for each state\n",
        "\n",
        "state_value[env.observation_space.n - 1] = 0  # assign the goal state as 0 since it's a terminal state\n",
        "\n",
        "for action in range(env.action_space.n):\n",
        "  q[env.observation_space.n - 1][action] = 0 # assign the goal state as 0 since it's a terminal state\n",
        "\n",
        "threshold = 0.1 # treshold for update for the action value function\n",
        "gamma = 0.9  # discount factor\n",
        "done = False\n",
        "\n",
        "\n",
        "while not done:\n",
        "  max_delta = 0\n",
        "  delta_q = []\n",
        "\n",
        "  # evaluating the state value function\n",
        "  for state in state_value:  # loop through each state\n",
        "    new_s_value = 0\n",
        "    for action, action_prob in enumerate(policy[state]):  # Looping throught the actions in the policy\n",
        "\n",
        "      action_value = 0\n",
        "      for prob, next_state, reward, _  in env.env.P[state][action]: # Looping through the next state probabilities and rewards\n",
        "        action_value += prob * (reward + gamma * state_value[next_state]) # Calcluating the state action value q(s,a)\n",
        "\n",
        "      new_s_value += action_prob * action_value # Calculating the state value from the action values\n",
        "      delta_q.append(abs(q[state][action] - action_value)) # Recording the difference between the old and new action-value function\n",
        "      q[state][action] = action_value # updating the state-action value\n",
        "\n",
        "    state_value[state] = new_s_value # updating the state value\n",
        "\n",
        "  max_delta = max(delta_q)\n",
        "  if max_delta < threshold: # checking if the maximum difference is less than the threshold\n",
        "    done = True # if, exit the loop\n",
        "\n",
        "print(\"policy evaluation finished! Here is the action value function q: \", q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_DtHXwTmjBxu",
        "outputId": "6e85a713-d108-43bd-f330-e1791adc9407"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "########################\n",
            "Policy improvement iteration:  1\n",
            "Policy evaluation iteration:  1\n",
            "Policy evaluation iteration:  2\n",
            "Policy evaluation iteration:  3\n",
            "Policy evaluation iteration:  4\n",
            "Policy evaluation iteration:  5\n",
            "Policy evaluation iteration:  6\n",
            "Policy evaluation iteration:  7\n",
            "Policy evaluation iteration:  8\n",
            "Policy evaluation iteration:  9\n",
            "Policy evaluation iteration:  10\n",
            "Policy evaluation iteration:  11\n",
            "Policy evaluation iteration:  12\n",
            "Policy evaluation iteration:  13\n",
            "Policy evaluation iteration:  14\n",
            "Policy evaluation iteration:  15\n",
            "Policy evaluation iteration:  16\n",
            "Policy evaluation iteration:  17\n",
            "Policy evaluation iteration:  18\n",
            "Policy evaluation iteration:  19\n",
            "Policy evaluation iteration:  20\n",
            "Policy evaluation iteration:  21\n",
            "Policy evaluation iteration:  22\n",
            "Policy evaluation iteration:  23\n",
            "Policy evaluation iteration:  24\n",
            "Policy evaluation iteration:  25\n",
            "Policy evaluation iteration:  26\n",
            "Policy evaluation iteration:  27\n",
            "Policy evaluation iteration:  28\n",
            "Policy evaluation iteration:  29\n",
            "Policy evaluation iteration:  30\n",
            "Policy evaluation iteration:  31\n",
            "Policy evaluation iteration:  32\n",
            "Policy evaluation iteration:  33\n",
            "Policy evaluation iteration:  34\n",
            "Policy evaluation iteration:  35\n",
            "Policy evaluation iteration:  36\n",
            "policy Iteration finished!\n",
            "########################\n",
            "Policy improvement iteration:  2\n",
            "Policy evaluation iteration:  1\n",
            "Policy evaluation iteration:  2\n",
            "Policy evaluation iteration:  3\n",
            "Policy evaluation iteration:  4\n",
            "Policy evaluation iteration:  5\n",
            "Policy evaluation iteration:  6\n",
            "Policy evaluation iteration:  7\n",
            "policy Iteration finished!\n",
            "########################\n",
            "Policy improvement iteration:  3\n",
            "Policy evaluation iteration:  1\n",
            "Policy evaluation iteration:  2\n",
            "policy Iteration finished!\n",
            "########################\n",
            "Policy improvement iteration:  4\n",
            "Policy evaluation iteration:  1\n",
            "policy Iteration finished!\n",
            "The optimal policy:  {0: array([0., 1., 0., 0.]), 1: array([0., 0., 1., 0.]), 2: array([0., 1., 0., 0.]), 3: array([1., 0., 0., 0.]), 4: array([0., 1., 0., 0.]), 5: array([1., 0., 0., 0.]), 6: array([0., 1., 0., 0.]), 7: array([1., 0., 0., 0.]), 8: array([0., 0., 1., 0.]), 9: array([0., 1., 0., 0.]), 10: array([0., 1., 0., 0.]), 11: array([1., 0., 0., 0.]), 12: array([1., 0., 0., 0.]), 13: array([0., 0., 1., 0.]), 14: array([0., 0., 1., 0.]), 15: array([1., 0., 0., 0.])}\n",
            "The optimal state-value function:  {0: 0.5904900000000002, 1: 0.6561000000000001, 2: 0.7290000000000001, 3: 0.6561000000000001, 4: 0.6561000000000001, 5: 0.0016154461876827031, 6: 0.81, 7: 0.0006387174406204384, 8: 0.7290000000000001, 9: 0.81, 10: 0.9, 11: 0.002959620424378664, 12: 0.0013967361341931568, 13: 0.9, 14: 1.0, 15: 0.0}\n"
          ]
        }
      ],
      "source": [
        "# Policy Improvement:\n",
        "# Question: How would you use P and your value function to improve an arbitrary\n",
        "# policy, pi, per Chapter 4?\n",
        "\n",
        "# Answer:\n",
        "\n",
        "# I would use policy improvement method to generate a better policy based on the\n",
        "# value function and the model of our environment (P).\n",
        "# The idea of policy improvement is that the action-value function will be\n",
        "# calculated for every action possible from each state then we choose the action\n",
        "# that gives the highest value to be the better action for that state by\n",
        "# improving our policy.\n",
        "\n",
        "\n",
        "\n",
        "# Practical: Code the policy iteration process, and employ it to arrive at a\n",
        "# policy that solves this problem. Show your testing results, and ensure\n",
        "# it reports the number of iterations for each step: (a) overall policy\n",
        "# iteration steps and (b) evaluation steps.\n",
        "\n",
        "\n",
        "# Answer:\n",
        "\n",
        "policy = {}\n",
        "state_value = {}\n",
        "\n",
        "# initializing our policy and state value functions\n",
        "for state in range(env.observation_space.n):\n",
        "  policy[state] = initialize_probabilities() # initialize policy to equal probabilities for each action\n",
        "  state_value[state] = random.random() # assign random state value for each state\n",
        "\n",
        "state_value[env.observation_space.n - 1] = 0  # assign the goal state as 0 since it's a terminal state\n",
        "\n",
        "\n",
        "threshold = 0.001  # treshold for update for the value function\n",
        "gamma = 0.9  # discount factor\n",
        "\n",
        "policy_iter = 0 # initializing iteration counter\n",
        "\n",
        "\n",
        "policy_stable = False\n",
        "while not policy_stable:\n",
        "  policy_iter += 1\n",
        "  print(\"########################\")\n",
        "  print(\"Policy improvement iteration: \", policy_iter)\n",
        "  # policy evaluation:\n",
        "\n",
        "  done = False\n",
        "  eval_iter = 0\n",
        "  while not done:\n",
        "    max_delta = 0\n",
        "    delta_s = []\n",
        "    # evaluating the state value function\n",
        "    for state in state_value:  # loop through each state\n",
        "      old_s_value = state_value[state]\n",
        "      new_s_value = 0\n",
        "      for action, action_prob in enumerate(policy[state]):  #Looping throught the actions in the policy\n",
        "        action_value = 0\n",
        "        for prob, next_state, reward, _  in env.env.P[state][action]: #looping through the next state probabilities and rewards\n",
        "          action_value += prob * (reward + gamma * state_value[next_state]) # calcluating the state action value q(s,a)\n",
        "        new_s_value += action_prob * action_value # calculating the state value from the action values\n",
        "\n",
        "      delta_s.append(abs(state_value[state] - new_s_value)) # recording the difference between the old and new state value\n",
        "      state_value[state] = new_s_value # updating the state value\n",
        "\n",
        "    max_delta = max(delta_s) # finding the maximum difference between the state values\n",
        "    if max_delta < threshold: # checking if the maximum difference is less than the threshold\n",
        "      done = True # for exiting the loop\n",
        "    eval_iter += 1\n",
        "    print(\"Policy evaluation iteration: \", eval_iter)\n",
        "  print(\"policy Iteration finished!\")\n",
        "\n",
        "\n",
        "  # Policy improvement\n",
        "\n",
        "\n",
        "  policy_stable = True\n",
        "  for state in range(env.observation_space.n): # looping through each state\n",
        "    old_action = policy[state] # storing the old action\n",
        "    action_values = [] # initializing the action values\n",
        "    for action in range(env.action_space.n):  # looping through each action\n",
        "      action_value = 0\n",
        "      for prob, next_state, reward, _  in env.env.P[state][action]: #looping through the next state probabilities and rewards\n",
        "        action_value += prob * (reward + gamma * state_value[next_state]) # calcluating the state action value q(s,a)\n",
        "\n",
        "      action_values.append(action_value)  # recording the value for q(s,a)\n",
        "\n",
        "    best_action = np.argmax(action_values) # selecting the action with higher value\n",
        "    policy[state] = np.zeros(env.action_space.n) # setting zero probabilities for all actions\n",
        "    policy[state][best_action] = 1 # setting the probability of the best action 1\n",
        "    if not np.array_equal(old_action, policy[state]): # checking if the old polcy is equal to the new policy at each state\n",
        "      policy_stable = False # if not, the policy is not stable\n",
        "\n",
        "\n",
        "print(\"The optimal policy: \", policy )\n",
        "print(\"The optimal state-value function: \", state_value)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "XoJ1J0ghjix9",
        "outputId": "70544255-778e-492e-9f9c-5113233fbc35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration:  1\n",
            "Iteration:  2\n",
            "Iteration:  3\n",
            "Iteration:  4\n",
            "Iteration:  5\n",
            "Iteration:  6\n",
            "Iteration:  7\n",
            "Iteration:  8\n",
            "Iteration:  9\n",
            "Iteration:  10\n",
            "Iteration:  11\n",
            "Iteration:  12\n",
            "Iteration:  13\n",
            "Iteration:  14\n",
            "Iteration:  15\n",
            "Iteration:  16\n",
            "Iteration:  17\n",
            "Iteration:  18\n",
            "Iteration:  19\n",
            "Iteration:  20\n",
            "Iteration:  21\n",
            "Iteration:  22\n",
            "Iteration:  23\n",
            "Iteration:  24\n",
            "Iteration:  25\n",
            "Iteration:  26\n",
            "Iteration:  27\n",
            "Iteration:  28\n",
            "Iteration:  29\n",
            "Iteration:  30\n",
            "Iteration:  31\n",
            "Iteration:  32\n",
            "Iteration:  33\n",
            "Iteration:  34\n",
            "Iteration:  35\n",
            "Iteration:  36\n",
            "Iteration:  37\n",
            "Iteration:  38\n",
            "Iteration:  39\n",
            "Iteration:  40\n",
            "Iteration:  41\n",
            "Iteration:  42\n",
            "Iteration:  43\n",
            "Iteration:  44\n",
            "Iteration:  45\n",
            "Optimal Policy:  {0: array([0., 1., 0., 0.]), 1: array([0., 0., 1., 0.]), 2: array([0., 1., 0., 0.]), 3: array([1., 0., 0., 0.]), 4: array([0., 1., 0., 0.]), 5: array([1., 0., 0., 0.]), 6: array([0., 1., 0., 0.]), 7: array([1., 0., 0., 0.]), 8: array([0., 0., 1., 0.]), 9: array([0., 1., 0., 0.]), 10: array([0., 1., 0., 0.]), 11: array([1., 0., 0., 0.]), 12: array([1., 0., 0., 0.]), 13: array([0., 0., 1., 0.]), 14: array([0., 0., 1., 0.]), 15: array([1., 0., 0., 0.])}\n",
            "Optimal State Value function:  {0: 0.5904900000000002, 1: 0.6561000000000001, 2: 0.7290000000000001, 3: 0.6561000000000001, 4: 0.6561000000000001, 5: 0.0016162495844119142, 6: 0.81, 7: 0.008537532836527051, 8: 0.7290000000000001, 9: 0.81, 10: 0.9, 11: 0.003214256253991719, 12: 0.005809452331157034, 13: 0.9, 14: 1.0, 15: 0.0}\n"
          ]
        }
      ],
      "source": [
        "# Practical: Code the value iteration process, and employ it to arrive at a\n",
        "# policy that solves this problem. Show your testing results, reporting\n",
        "# the iteration counts.\n",
        "# Comment on the difference between the iterations required for policy vs\n",
        "# value iteration.\n",
        "#\n",
        "\n",
        "# Answer: Here is value iteration for the current problem\n",
        "\n",
        "policy = {}\n",
        "state_value = {}\n",
        "\n",
        "# initializing our policy and state value functions\n",
        "for state in range(env.observation_space.n):\n",
        "  policy[state] = initialize_probabilities() # initialize policy to equal probabilities for each action\n",
        "  state_value[state] = random.random() # assign random state value for each state\n",
        "\n",
        "state_value[env.observation_space.n - 1] = 0  # assign the goal state as 0 since it's a terminal state\n",
        "\n",
        "\n",
        "threshold = 0.001  # treshold for update for the value function\n",
        "gamma = 0.9  # discount factor\n",
        "done = False\n",
        "val_iter = 0\n",
        "\n",
        "while not done:\n",
        "  val_iter += 1\n",
        "  print(\"Iteration: \", val_iter)\n",
        "  max_delta = 0\n",
        "  delta_s = []\n",
        "  # evaluating the state value function\n",
        "  for state in state_value:  # loop through each state\n",
        "    old_s_value = state_value[state] # old state value for comparison\n",
        "    new_s_value = 0\n",
        "    action_values = []\n",
        "    for action, action_prob in enumerate(policy[state]):  #Looping throught the actions in the policy\n",
        "      action_value = 0\n",
        "\n",
        "      for prob, next_state, reward, _  in env.env.P[state][action]: #looping through the next state probabilities and rewards\n",
        "        action_value += prob * (reward + gamma * state_value[next_state])\n",
        "\n",
        "      action_values.append(action_value)  # recording the value for q(s,a)\n",
        "      new_s_value = max(action_values) # selecting the action with higher value\n",
        "\n",
        "    best_action = np.argmax(action_values) # selecting the action with higher value\n",
        "\n",
        "    # updating the policy\n",
        "    policy[state] = np.zeros(env.action_space.n) # setting zero probabilities for all actions\n",
        "    policy[state][best_action] = 1 # setting the probability of the best action 1\n",
        "\n",
        "    delta_s.append(abs(state_value[state] - new_s_value)) # recording the difference between the old and new state value\n",
        "    state_value[state] = new_s_value # updating the state value\n",
        "  max_delta = max(delta_s) # finding the maximum difference between the state values\n",
        "  if max_delta < threshold: # checking if the maximum difference is less than the threshold\n",
        "    done = True\n",
        "\n",
        "# printing the optimal policy and value function\n",
        "print(\"Optimal Policy: \", policy)\n",
        "print(\"Optimal State Value function: \", state_value)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2cdJivwajm0B",
        "outputId": "d1aad2a5-8b5a-48ee-bb11-35000457125b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration:  1\n",
            "Iteration:  2\n",
            "Iteration:  3\n",
            "Iteration:  4\n",
            "Iteration:  5\n",
            "Iteration:  6\n",
            "Iteration:  7\n",
            "Iteration:  8\n",
            "Iteration:  9\n",
            "Iteration:  10\n",
            "Iteration:  11\n",
            "Iteration:  12\n",
            "Iteration:  13\n",
            "Iteration:  14\n",
            "Iteration:  15\n",
            "Iteration:  16\n",
            "Iteration:  17\n",
            "Iteration:  18\n",
            "Iteration:  19\n",
            "Iteration:  20\n",
            "Iteration:  21\n",
            "Iteration:  22\n",
            "Iteration:  23\n",
            "Iteration:  24\n",
            "Iteration:  25\n",
            "Iteration:  26\n",
            "Iteration:  27\n",
            "Iteration:  28\n",
            "Iteration:  29\n",
            "Iteration:  30\n",
            "Iteration:  31\n",
            "Iteration:  32\n",
            "Iteration:  33\n",
            "Iteration:  34\n",
            "Iteration:  35\n",
            "Iteration:  36\n",
            "Iteration:  37\n",
            "Iteration:  38\n",
            "Iteration:  39\n",
            "Iteration:  40\n",
            "Iteration:  41\n",
            "Iteration:  42\n",
            "Iteration:  43\n",
            "Iteration:  44\n",
            "Optimal Policy:  {0: array([1., 0., 0., 0.]), 1: array([0., 0., 0., 1.]), 2: array([1., 0., 0., 0.]), 3: array([0., 0., 0., 1.]), 4: array([1., 0., 0., 0.]), 5: array([1., 0., 0., 0.]), 6: array([1., 0., 0., 0.]), 7: array([1., 0., 0., 0.]), 8: array([0., 0., 0., 1.]), 9: array([0., 1., 0., 0.]), 10: array([1., 0., 0., 0.]), 11: array([1., 0., 0., 0.]), 12: array([1., 0., 0., 0.]), 13: array([0., 0., 1., 0.]), 14: array([0., 1., 0., 0.]), 15: array([1., 0., 0., 0.])}\n",
            "Optimal State Value function:  {0: 0.07008073010630185, 1: 0.06342108425218171, 2: 0.07730286996473824, 3: 0.058452090285144795, 4: 0.09292974270096811, 5: 0.008502602081800028, 6: 0.11616132368508694, 7: 0.00014833725049623903, 8: 0.1464625049927093, 9: 0.24854797272354445, 10: 0.3012134280332527, 11: 0.0034479740141532887, 12: 0.0009053095282409013, 13: 0.38055457008233284, 14: 0.6393003434686964, 15: 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "# Optional: instead of the above environment, use the \"slippery\" Frozen Lake via\n",
        "env = gym.make(\"FrozenLake-v1\")\n",
        "\n",
        "# Answer:\n",
        "\n",
        "\n",
        "policy = {}\n",
        "state_value = {}\n",
        "\n",
        "# initializing our policy and state value functions\n",
        "for state in range(env.observation_space.n):\n",
        "  policy[state] = initialize_probabilities() # initialize policy to equal probabilities for each action\n",
        "  state_value[state] = random.random() # assign random state value for each state\n",
        "\n",
        "state_value[env.observation_space.n - 1] = 0  # assign the goal state as 0 since it's a terminal state\n",
        "\n",
        "\n",
        "threshold = 0.001  # treshold for update for the value function\n",
        "gamma = 0.9  # discount factor\n",
        "done = False\n",
        "val_iter = 0 # initializing iteration counter\n",
        "\n",
        "while not done:\n",
        "  val_iter += 1\n",
        "  print(\"Iteration: \", val_iter)\n",
        "  max_delta = 0\n",
        "  delta_s = []\n",
        "  # evaluating the state value function\n",
        "  for state in state_value:  # loop through each state\n",
        "    old_s_value = state_value[state] # old state value for comparison\n",
        "    new_s_value = 0\n",
        "    action_values = []\n",
        "    for action, action_prob in enumerate(policy[state]):  #Looping throught the actions in the policy\n",
        "      action_value = 0\n",
        "      for prob, next_state, reward, _  in env.env.P[state][action]:\n",
        "        #looping through the next state probabilities and rewards\n",
        "\n",
        "        action_value += prob * (reward + gamma * state_value[next_state]) # calcluating the state action value q(s,a)\n",
        "\n",
        "      action_values.append(action_value)  # recording the value for q(s,a)\n",
        "      new_s_value = max(action_values)  # selecting the action with higher value\n",
        "\n",
        "    best_action = np.argmax(action_values) # selecting the action with higher value\n",
        "\n",
        "    # updating the policy\n",
        "    policy[state] = np.zeros(env.action_space.n)  # setting zero probabilities for all actions\n",
        "    policy[state][best_action] = 1  # setting the probability of the best action 1\n",
        "\n",
        "    delta_s.append(abs(state_value[state] - new_s_value)) # recording the difference between the old and new state value\n",
        "    state_value[state] = new_s_value # updating the state value\n",
        "  max_delta = max(delta_s)\n",
        "  if max_delta < threshold:\n",
        "    done = True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Optimal Policy: \", policy)\n",
        "print(\"Optimal State Value function: \", state_value)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (dp)",
      "language": "python",
      "name": "dp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
